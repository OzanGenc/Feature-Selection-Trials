{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import pylab \n",
    "import scipy.stats as stats\n",
    "import time\n",
    "import sklearn.ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from pandas import ExcelWriter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "\n",
    "import collections\n",
    "\n",
    "import winsound\n",
    "frequency = 2500  # Set Frequency To 2500 Hertz\n",
    "duration = 1000  # Set Duration To 1000 ms == 1 second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd C:\\Users\\DELL003\\Desktop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('All_New.xlsx', sheet_name = 'All')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seperating the groups\n",
    "group0 = data[data.grup==0]\n",
    "group1 = data[data.grup==1]\n",
    "group2 = data[data.grup==2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose only ONE from below options for groups\n",
    "-----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For All groups\n",
    "data = pd.concat([group0, group1, group2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Controls and Low Performers\n",
    "data = pd.concat([group0, group2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Controls and High Performers\n",
    "data = pd.concat([group0, group1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For High Performers and Low Performers\n",
    "data = pd.concat([group1, group2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seperating Modalities\n",
    "aBV = data.filter(like='aBV')\n",
    "CBF = data.filter(like='CBF')\n",
    "SPECTROSCOPY = data.filter(like='SPECTROSCOPY')\n",
    "grup = data['grup']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose only ONE from below options for modalities\n",
    " ----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For CBF\n",
    "data = pd.concat([grup, CBF], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For aBV\n",
    "data = pd.concat([grup, aBV], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For ASL\n",
    "data = pd.concat([grup, CBF, aBV], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For SPECTROSCOPY\n",
    "data = pd.concat([grup, SPECTROSCOPY], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In practice, feature selection should be done after data pre-processing,\n",
    "# so ideally, all the categorical variables are encoded into numbers,\n",
    "# and then you can assess how deterministic they are of the target\n",
    "\n",
    "# here for simplicity I will use only numerical variables\n",
    "# select numerical columns:\n",
    "\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numerical_vars = list(data.select_dtypes(include=numerics).columns)\n",
    "data = data[numerical_vars]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping NA\n",
    "data = data.dropna(axis = 1, thresh = round((0.8)*data.shape[0])) #removing features existing in less than 0.2 of the samples \n",
    "data = data.dropna(axis = 0, thresh = round((0.5)*data.shape[1])) #removing samples having features less than half of the total features \n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking missing values in features\n",
    "pd.options.display.max_rows = 4000\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change number of selected features\n",
    "number_of_selected_features = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer1 = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer2 = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Selection Algorithms\n",
    "fs1 = RFE(RandomForestClassifier(n_estimators=100), n_features_to_select=number_of_selected_features, verbose=2)\n",
    "fs2 = RFE(ExtraTreesClassifier(n_estimators=100, bootstrap=True), n_features_to_select=number_of_selected_features, verbose=2)\n",
    "fs3 = RFE(GradientBoostingClassifier(n_estimators=100, max_features='auto', subsample=0.7), n_features_to_select=number_of_selected_features, verbose=2)\n",
    "fs4 = RFE(AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=3), n_estimators=100), n_features_to_select=number_of_selected_features, verbose=2)\n",
    "#Add more feature selection options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipelines\n",
    "\n",
    "pipe1 = Pipeline([('imputer', imputer1), ('scaler', StandardScaler()),\n",
    "                 ('fs', fs1)])\n",
    "\n",
    "pipe2 = Pipeline([('imputer', imputer2), ('scaler', StandardScaler()),\n",
    "                 ('fs', fs1)])\n",
    "\n",
    "\n",
    "pipe3 = Pipeline([('imputer', imputer1), ('scaler', StandardScaler()),\n",
    "                 ('fs', fs2)])\n",
    "\n",
    "\n",
    "pipe4 = Pipeline([('imputer', imputer2), ('scaler', StandardScaler()),\n",
    "                 ('fs', fs2)])\n",
    "\n",
    "\n",
    "pipe5 = Pipeline([('imputer', imputer1), ('scaler', StandardScaler()),\n",
    "                 ('fs', fs3)])\n",
    "\n",
    "\n",
    "pipe6 = Pipeline([('imputer', imputer2), ('scaler', StandardScaler()),\n",
    "                 ('fs', fs3)])\n",
    "\n",
    "\n",
    "\n",
    "pipe7 = Pipeline([('imputer', imputer1), ('scaler', StandardScaler()),\n",
    "                 ('fs', fs4)])\n",
    "\n",
    "\n",
    "pipe8 = Pipeline([('imputer', imputer2), ('scaler', StandardScaler()),\n",
    "                 ('fs', fs4)])\n",
    "\n",
    "#Add more pipelines\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelist = [pipe1, pipe2, pipe3, pipe4, pipe5, pipe6]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking training set model performance for detecting underfitting\n",
    "start = time.time()\n",
    "for pipe in pipelist:\n",
    "    pipe = pipe.fit(np.array(data.drop(labels=['grup'], axis=1)), data['grup'])\n",
    "    training_pred = pipe.predict(np.array(data.drop(labels=['grup'], axis=1)))\n",
    "    print(confusion_matrix(data['grup'],training_pred))\n",
    "\n",
    "end = time.time()\n",
    "print('Computation Time:',end - start)\n",
    "winsound.Beep(frequency, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Estimated_Time = ((end - start)*100)\n",
    "print('Estimated Time:',Estimated_Time/60,'min','or',Estimated_Time/3600,'h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "print('Program start to run at',time.localtime())\n",
    "counter_collections=collections.Counter()\n",
    "\n",
    "for pipe in pipelist:\n",
    "    selectedfeatures=[]\n",
    "\n",
    "    for i in range(100):\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(np.array(data.drop(labels=['grup'], axis=1)), data['grup'], test_size=0.2)\n",
    "\n",
    "\n",
    "        # run grid search\n",
    "        pipe = pipe.fit(np.array(X_train), y_train)\n",
    "        selectedfeatures.append(tuple(pipe.steps[2][1].get_support(indices=True)))          #for RF-RFE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    counter=collections.Counter(selectedfeatures)\n",
    "    counter_collections = counter_collections + counter\n",
    "    \n",
    "    print('Most Common Selected Features:',np.array(counter.most_common))    #change according to number of selected features\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "print('Total Selected Features in all methods:',np.array(counter_collections.most_common))\n",
    "\n",
    "end = time.time()\n",
    "print('Computation Time:',(end - start)/60,'min')\n",
    "winsound.Beep(frequency, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data.drop(labels=['grup'], axis=1).iloc[:,[92,14,4,53,60,2]]\n",
    "features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot('aBVmedian331', # Horizontal axis\n",
    "           'aBVmedian207', # Vertical axis\n",
    "           data=data, # Data source\n",
    "           fit_reg=False, # Don't fix a regression line\n",
    "           hue=\"grup\", # Set color\n",
    "           scatter_kws={\"marker\": \"D\", # Set marker style\n",
    "                        \"s\": 40}) # S marker size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = features.corr()\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(corr,cmap='coolwarm', vmin=-1, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "ticks = np.arange(0,len(features.columns),1)\n",
    "ax.set_xticks(ticks)\n",
    "plt.xticks(rotation=90)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xticklabels(features.columns)\n",
    "ax.set_yticklabels(features.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_with_grup = pd.concat([features,data['grup']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_with_grup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd C:\\Users\\DELL003\\Desktop\\Output_Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = ExcelWriter('featuresCBFmeanAll_KH.xlsx')\n",
    "pd.DataFrame(selected_features_with_grup).to_excel(writer,'Sheet1', index=False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
